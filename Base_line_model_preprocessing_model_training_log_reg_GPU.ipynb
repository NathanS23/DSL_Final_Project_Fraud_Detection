{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUJlkP1bP2hv",
    "outputId": "ea2cab97-6f19-41bc-9a21-a8e6adf83550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8gIGzDPAdq6"
   },
   "outputs": [],
   "source": [
    "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q1bqoEdaOTqm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory reduction function\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Mem. usage decreased to {end_mem:.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n",
    "# Combined preprocessing and UID detection\n",
    "def preprocess_and_detect_uid(train_df, test_df, train_identity=None, test_identity=None):\n",
    "    print(\"Starting preprocessing...\")\n",
    "\n",
    "    # Memory reduction\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "    # Merge with identity datasets if provided\n",
    "    if train_identity is not None and test_identity is not None:\n",
    "        train_df = pd.merge(train_df, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "        test_df = pd.merge(test_df, test_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "    # Add full address feature\n",
    "    train_df['full_addr'] = train_df['addr1'].astype(str) + '_' + train_df['addr2'].astype(str)\n",
    "    test_df['full_addr'] = test_df['addr1'].astype(str) + '_' + test_df['addr2'].astype(str)\n",
    "\n",
    "    # Add time-based UID features\n",
    "    for col in ['D1', 'D2', 'D3', 'D5', 'D10', 'D11', 'D15']:\n",
    "        if col in train_df.columns:\n",
    "            new_col = 'uid_td_' + col\n",
    "            train_df[new_col] = train_df['TransactionDT'] / (24 * 60 * 60)\n",
    "            train_df[new_col] = np.floor(train_df[new_col] - train_df[col]) + 1000\n",
    "\n",
    "            test_df[new_col] = test_df['TransactionDT'] / (24 * 60 * 60)\n",
    "            test_df[new_col] = np.floor(test_df[new_col] - test_df[col]) + 1000\n",
    "\n",
    "    # Add normalized day feature\n",
    "    train_df['DT_day'] = np.floor(train_df['TransactionDT'] / (24 * 60 * 60)) + 1000\n",
    "    test_df['DT_day'] = np.floor(test_df['TransactionDT'] / (24 * 60 * 60)) + 1000\n",
    "\n",
    "    # Round transaction amounts for feature creation\n",
    "    train_df['TransactionAmt_fix'] = np.round(train_df['TransactionAmt'], 2)\n",
    "    test_df['TransactionAmt_fix'] = np.round(test_df['TransactionAmt'], 2)\n",
    "\n",
    "    # UID detection\n",
    "    train_df['uid'] = train_df['card1'].astype(str) + '_' + train_df['card2'].astype(str) + '_' + train_df['addr1'].astype(str)\n",
    "    test_df['uid'] = test_df['card1'].astype(str) + '_' + test_df['card2'].astype(str) + '_' + test_df['addr1'].astype(str)\n",
    "\n",
    "    # Frequency encoding for UID\n",
    "    uid_freq = pd.concat([train_df['uid'], test_df['uid']]).value_counts()\n",
    "    train_df['uid_count'] = train_df['uid'].map(uid_freq)\n",
    "    test_df['uid_count'] = test_df['uid'].map(uid_freq)\n",
    "\n",
    "    print(\"Preprocessing and UID detection complete.\")\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qpcdTnzuP0nO"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('drive/MyDrive/DSC_final_project/train_transaction.csv')\n",
    "train_identity = pd.read_csv('drive/MyDrive/DSC_final_project/train_identity.csv')\n",
    "test_df = pd.read_csv('drive/MyDrive/DSC_final_project/test_transaction.csv')\n",
    "test_identity = pd.read_csv('drive/MyDrive/DSC_final_project/test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBW0HCnJt1NC",
    "outputId": "e69f6120-6591-4277-a3a6-b6c8097f7949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 472.59 Mb (68.9% reduction)\n",
      "Preprocessing and UID detection complete.\n"
     ]
    }
   ],
   "source": [
    "train_processed, test_processed = preprocess_and_detect_uid(train_df, test_df, train_identity, test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z4R4faSmRjXW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def preprocess_for_baseline_with_onehot_sparse(input_train_df, input_test_df, target_column='IsFraud'):\n",
    "    # Work on local copies to avoid modifying global variables\n",
    "    train_df = input_train_df.copy()\n",
    "    test_df = input_test_df.copy()\n",
    "\n",
    "    # Drop target column from training data\n",
    "    if target_column in train_df.columns:\n",
    "        train_df = train_df.drop(columns=[target_column])\n",
    "\n",
    "    # Identify numeric and categorical columns that exist in the datasets\n",
    "    numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_columns = train_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Ensure the columns exist in both train and test\n",
    "    numeric_columns = [col for col in numeric_columns if col in test_df.columns]\n",
    "    categorical_columns = [col for col in categorical_columns if col in test_df.columns]\n",
    "\n",
    "    # Handle missing values\n",
    "    train_df[numeric_columns] = train_df[numeric_columns].fillna(train_df[numeric_columns].median())\n",
    "    test_df[numeric_columns] = test_df[numeric_columns].fillna(test_df[numeric_columns].median())\n",
    "    train_df[categorical_columns] = train_df[categorical_columns].fillna('missing')\n",
    "    test_df[categorical_columns] = test_df[categorical_columns].fillna('missing')\n",
    "\n",
    "    # OneHotEncoder for categorical columns\n",
    "    if categorical_columns:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')  # Use sparse matrices\n",
    "        train_categorical_sparse = ohe.fit_transform(train_df[categorical_columns])\n",
    "        test_categorical_sparse = ohe.transform(test_df[categorical_columns])\n",
    "\n",
    "        # Save memory by retaining sparse matrices\n",
    "        train_df = train_df.drop(columns=categorical_columns)\n",
    "        test_df = test_df.drop(columns=categorical_columns)\n",
    "\n",
    "    # Standardize numeric columns\n",
    "    if numeric_columns:\n",
    "        scaler = StandardScaler()\n",
    "        train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "        test_df[numeric_columns] = scaler.transform(test_df[numeric_columns])\n",
    "\n",
    "    # Convert numeric columns to sparse format and concatenate\n",
    "    if numeric_columns:\n",
    "        train_numeric_sparse = sp.csr_matrix(train_df[numeric_columns].values.astype(np.float32))\n",
    "        test_numeric_sparse = sp.csr_matrix(test_df[numeric_columns].values.astype(np.float32))\n",
    "\n",
    "        train_sparse = sp.hstack([train_numeric_sparse, train_categorical_sparse], format='csr')\n",
    "        test_sparse = sp.hstack([test_numeric_sparse, test_categorical_sparse], format='csr')\n",
    "    else:\n",
    "        train_sparse = train_categorical_sparse\n",
    "        test_sparse = test_categorical_sparse\n",
    "\n",
    "    return train_sparse, test_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7qLiLxt4wvp",
    "outputId": "68b4d557-3815-4e7c-c60c-4347bbe90e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OZmAfYVSX1lV"
   },
   "outputs": [],
   "source": [
    "prepocessed_train, prepocessed_test = preprocess_for_baseline_with_onehot_sparse(train_processed, test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSbpd3HcX-9g",
    "outputId": "ed3602f1-628f-4ec9-9529-9141051b1ae6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:48:29,737] A new study created in memory with name: GPU Logistic Regression Tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34131738696261177\n",
      "0.34239190083931176\n",
      "0.3419132316833835\n",
      "0.3426304954744556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:49:01,933] Trial 0 finished with value: 0.34279517543590854 and parameters: {'C': 1.01974693075021}. Best is trial 0 with value: 0.34279517543590854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34279517543590854\n",
      "0.5423706705689512\n",
      "0.5436765141459714\n",
      "0.5418103523528651\n",
      "0.5421215506057802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:49:04,745] Trial 1 finished with value: 0.542082529078044 and parameters: {'C': 0.0003798170803813578}. Best is trial 0 with value: 0.34279517543590854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.542082529078044\n",
      "0.524332550162172\n",
      "0.5255865391604699\n",
      "0.5238089754980525\n",
      "0.5241377068823354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:49:08,835] Trial 2 finished with value: 0.5240172889562312 and parameters: {'C': 0.0015206657059152293}. Best is trial 0 with value: 0.34279517543590854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5240172889562312\n",
      "0.5530617154143186\n",
      "0.5543585477337452\n",
      "0.5525429869472152\n",
      "0.5528422895021006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:49:10,834] Trial 3 finished with value: 0.5528319916233693 and parameters: {'C': 0.00012380435805592387}. Best is trial 0 with value: 0.34279517543590854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5528319916233693\n",
      "0.3323631435453769\n",
      "0.33329342072318413\n",
      "0.33281221895952334\n",
      "0.3334859214261799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:49:51,893] Trial 4 finished with value: 0.3337574715007993 and parameters: {'C': 3.2630018283171065}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3337574715007993\n",
      "0.459134745275502\n",
      "0.46049079493125533\n",
      "0.45902838284708086\n",
      "0.4594165649148779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:50:00,067] Trial 5 finished with value: 0.4591334236035164 and parameters: {'C': 0.018725874984155462}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591334236035164\n",
      "0.5435457388690567\n",
      "0.5448655813372146\n",
      "0.5430198807671651\n",
      "0.5433453850074158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:50:02,705] Trial 6 finished with value: 0.5433286683091199 and parameters: {'C': 0.0003339021920351738}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5433286683091199\n",
      "0.4029764542959232\n",
      "0.40401890991743944\n",
      "0.40304004648185604\n",
      "0.4036594683626384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:50:16,589] Trial 7 finished with value: 0.40346242556842765 and parameters: {'C': 0.08780659878691065}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40346242556842765\n",
      "0.44020551772909233\n",
      "0.44134749653267913\n",
      "0.43987112851762494\n",
      "0.44048108915888656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:50:26,828] Trial 8 finished with value: 0.44022465727124027 and parameters: {'C': 0.03179939398625025}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44022465727124027\n",
      "0.5421813597768301\n",
      "0.5435136789509618\n",
      "0.5416640691871586\n",
      "0.5419964967591778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:50:29,698] Trial 9 finished with value: 0.5419718725590539 and parameters: {'C': 0.0003839211814373898}. Best is trial 4 with value: 0.3337574715007993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5419718725590539\n",
      "0.33213982623007443\n",
      "0.33245041836116185\n",
      "0.3321308975505203\n",
      "0.3328353203993377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:51:29,612] Trial 10 finished with value: 0.33314724297784404 and parameters: {'C': 40.864817467513994}. Best is trial 10 with value: 0.33314724297784404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33314724297784404\n",
      "0.3320909269962667\n",
      "0.33244488823492835\n",
      "0.3324241051195297\n",
      "0.3331939808200488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:52:32,239] Trial 11 finished with value: 0.3335528535729252 and parameters: {'C': 56.958407270148115}. Best is trial 10 with value: 0.33314724297784404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3335528535729252\n",
      "0.3322659734684514\n",
      "0.3330359640542784\n",
      "0.33268020786901703\n",
      "0.3332435467895212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:53:40,853] Trial 12 finished with value: 0.33378008942094745 and parameters: {'C': 78.56796146426251}. Best is trial 10 with value: 0.33314724297784404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33378008942094745\n",
      "0.33155518042842486\n",
      "0.33205988317720786\n",
      "0.3317900519920378\n",
      "0.33247565169674076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:54:35,997] Trial 13 finished with value: 0.3329529804087489 and parameters: {'C': 42.85890095129514}. Best is trial 13 with value: 0.3329529804087489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3329529804087489\n",
      "0.33095166285936584\n",
      "0.33180816948140246\n",
      "0.33149857885943124\n",
      "0.3322780237950773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:55:22,972] Trial 14 finished with value: 0.33258721210939185 and parameters: {'C': 4.485989414870252}. Best is trial 14 with value: 0.33258721210939185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33258721210939185\n",
      "0.33100999481912097\n",
      "0.33208499033060634\n",
      "0.33169108653888135\n",
      "0.3324039419569283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:56:06,486] Trial 15 finished with value: 0.3327363756026286 and parameters: {'C': 4.373708550421695}. Best is trial 14 with value: 0.33258721210939185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3327363756026286\n",
      "0.33304114828307774\n",
      "0.33402101642539\n",
      "0.33381573982657425\n",
      "0.33466190273304497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:56:45,657] Trial 16 finished with value: 0.3349150317571402 and parameters: {'C': 2.521610413748298}. Best is trial 14 with value: 0.33258721210939185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3349150317571402\n",
      "0.33027674084955\n",
      "0.3309702902125936\n",
      "0.33071998484902454\n",
      "0.3314977431849833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:57:32,568] Trial 17 finished with value: 0.3318781557968956 and parameters: {'C': 7.523470284010317}. Best is trial 17 with value: 0.3318781557968956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3318781557968956\n",
      "0.3678279997668923\n",
      "0.36903353472062717\n",
      "0.3685479855929628\n",
      "0.3692728424075452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:57:53,701] Trial 18 finished with value: 0.36926760198165354 and parameters: {'C': 0.26361324769032213}. Best is trial 17 with value: 0.3318781557968956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36926760198165354\n",
      "0.33057453024747585\n",
      "0.3310828940586367\n",
      "0.3307358527172471\n",
      "0.33150242241198435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:58:46,346] Trial 19 finished with value: 0.33191341496367394 and parameters: {'C': 12.121541808000616}. Best is trial 17 with value: 0.3318781557968956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33191341496367394\n",
      "Best trial:\n",
      "{'C': 7.523470284010317}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import cupy as cp\n",
    "import cupyx.scipy.sparse as cps\n",
    "from cuml.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Target variable\n",
    "y_train = train_processed['isFraud']\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    param = {\n",
    "        \"C\": trial.suggest_loguniform(\"C\", 1e-4, 1e2),  # Regularization strength\n",
    "        \"solver\": \"qn\",  # Use quasi-Newton solver (cuML only supports this)\n",
    "        \"penalty\": \"l2\",  # L2 regularization only\n",
    "        \"class_weight\": \"balanced\",  # Handle imbalanced data\n",
    "        \"max_iter\": 10000,\n",
    "        \"tol\": 1e-4\n",
    "    }\n",
    "\n",
    "    # Logistic Regression using cuML (GPU)\n",
    "    model = LogisticRegression(**param)\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    log_losses = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(prepocessed_train, y_train):\n",
    "        # Convert to GPU sparse matrices\n",
    "        X_train = cps.csr_matrix(prepocessed_train[train_idx])\n",
    "        X_val = cps.csr_matrix(prepocessed_train[val_idx])\n",
    "        y_train_fold = cp.array(y_train.iloc[train_idx])\n",
    "        y_val_fold = cp.array(y_train.iloc[val_idx])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train_fold)\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Calculate log loss\n",
    "        log_losses.append(log_loss(y_val_fold.get(), y_val_pred_proba.get()))\n",
    "        print(np.mean(log_losses))\n",
    "    # Return mean log loss\n",
    "    return np.mean(log_losses)\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"GPU Logistic Regression Tuning\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Best trial parameters\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# Cross-validation with Best Parameters\n",
    "best_params = study.best_trial.params\n",
    "model = LogisticRegression(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwT2E_M9M6mq",
    "outputId": "3393c76c-8b80-4b35-ceea-c9e7bd04018e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [23:00:34.943146] L-BFGS: max iterations reached\n",
      "[W] [23:00:34.944683] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:00:41.482084] L-BFGS: max iterations reached\n",
      "[W] [23:00:41.483473] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:00:47.090894] L-BFGS: max iterations reached\n",
      "[W] [23:00:47.092088] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:00:53.070335] L-BFGS: max iterations reached\n",
      "[W] [23:00:53.071594] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:00:58.751331] L-BFGS: max iterations reached\n",
      "[W] [23:00:58.754100] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "Final Cross-Validation Metrics with Best Parameters:\n",
      "Mean Log Loss: 0.0987\n",
      "Mean AUC Score: 0.9007\n",
      "Mean F1 Score: 0.3395\n",
      "Mean Precision: 0.7359\n",
      "Mean Recall: 0.2206\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with Best Parameters\n",
    "best_params = study.best_trial.params\n",
    "model = LogisticRegression(**best_params)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "log_losses = []\n",
    "\n",
    "for train_idx, val_idx in cv.split(prepocessed_train, y_train):\n",
    "    X_train = cps.csr_matrix(prepocessed_train[train_idx])\n",
    "    X_val = cps.csr_matrix(prepocessed_train[val_idx])\n",
    "    y_train_fold = cp.array(y_train.iloc[train_idx])\n",
    "    y_val_fold = cp.array(y_train.iloc[val_idx])\n",
    "\n",
    "    # Train with best parameters\n",
    "    model.fit(X_train, y_train_fold)\n",
    "\n",
    "    # Predict probabilities and classes\n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = (y_val_pred_proba > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "    # Metrics\n",
    "    log_losses.append(log_loss(y_val_fold.get(), y_val_pred_proba.get()))\n",
    "    auc_scores.append(roc_auc_score(y_val_fold.get(), y_val_pred_proba.get()))\n",
    "    f1_scores.append(f1_score(y_val_fold.get(), y_val_pred.get()))\n",
    "    precision_scores.append(precision_score(y_val_fold.get(), y_val_pred.get()))\n",
    "    recall_scores.append(recall_score(y_val_fold.get(), y_val_pred.get()))\n",
    "\n",
    "# Print Final Metrics\n",
    "print(\"Final Cross-Validation Metrics with Best Parameters:\")\n",
    "print(f\"Mean Log Loss: {np.mean(log_losses):.4f}\")\n",
    "print(f\"Mean AUC Score: {np.mean(auc_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ldqr9FdIrNgy"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "# Save sparse matrices\n",
    "save_npz('drive/MyDrive/DSC_final_project/train_sparse_BASELINE_LOGISTIC.npz', prepocessed_train)\n",
    "save_npz('drive/MyDrive/DSC_final_project/test_sparse_BASELINE_LOGISTIC.npz', prepocessed_test)\n",
    "\n",
    "# Load sparse matrices\n",
    "train_sparse = load_npz('drive/MyDrive/DSC_final_project/train_sparse_BASELINE_LOGISTIC.npz')\n",
    "test_sparse = load_npz('drive/MyDrive/DSC_final_project/test_sparse_BASELINE_LOGISTIC.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KydlhprQs0RS"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(cps.csr_matrix(prepocessed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vHWNMxIOm5U"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"TransactionID\": test_df[\"TransactionID\"],  # Replace with your test set's TransactionID column\n",
    "    \"isFraud\": predicted_proba  # Convert CuPy array to NumPy\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"drive/MyDrive/DSC_final_project/log_reg_initial_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
